{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare analyzers on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how saliency maps are computed for various methods on the MNIST dataset. We will first train a simple 3-layer MLP and then apply different methods. In the end we allow for a qualitive assesment by plotting the analysis as a grid.\n",
    "\n",
    "Parts of the code that do not contribute to the main focus are outsourced into utility modules. To learn more about the basic usage of **iNNvestigate** have look into this notebook: [Introduction to iNNvestigate](introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from functools import partial\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.models\n",
    "\n",
    "\n",
    "\n",
    "# Use utility libraries to focus on relevant iNNvestigate routines.\n",
    "eutils = imp.load_source(\"utils\", \"../utils.py\")\n",
    "mnistutils = imp.load_source(\"utils_mnist\", \"../utils_mnist.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from innvestigate.analyzer import ReverseAnalyzerBase\n",
    "from innvestigate.analyzer.relevance_based.relevance_analyzer import DeepTaylor\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.keras.checks as kchecks\n",
    "import innvestigate.utils.keras as kutils\n",
    "\n",
    "\n",
    "import innvestigate.utils.keras.graph as kgraph\n",
    "\n",
    "\n",
    "\n",
    "import innvestigate.layers as ilayers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Load the dataset and keep some images from the test set for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# returns x_train, y_train, x_test, y_test as numpy.ndarray\n",
    "data_not_preprocessed = mnistutils.fetch_data()\n",
    "\n",
    "# Create preprocessing functions\n",
    "input_range = [-1, 1]\n",
    "preprocess, revert_preprocessing = mnistutils.create_preprocessing_f(data_not_preprocessed[0], input_range)\n",
    "\n",
    "# Preprocess data\n",
    "data = (\n",
    "    preprocess(data_not_preprocessed[0]), data_not_preprocessed[1],\n",
    "    preprocess(data_not_preprocessed[2]), data_not_preprocessed[3]\n",
    ")\n",
    "\n",
    "num_classes = len(np.unique(data[1]))\n",
    "label_to_class_name = [str(i) for i in range(num_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The next part trains and evaluates a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.1995 - acc: 0.9408\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 97s 2ms/step - loss: 0.0439 - acc: 0.9866\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 95s 2ms/step - loss: 0.0269 - acc: 0.9919\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 95s 2ms/step - loss: 0.0193 - acc: 0.9938\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 100s 2ms/step - loss: 0.0130 - acc: 0.9957\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0075 - acc: 0.9975\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0071 - acc: 0.9974\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0037 - acc: 0.9990\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 95s 2ms/step - loss: 0.0054 - acc: 0.9985\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.0045 - acc: 0.9986\n",
      "Scores on test set: loss=0.037167854320569675 accuracy=0.9901\n"
     ]
    }
   ],
   "source": [
    "# Create & train model\n",
    "if keras.backend.image_data_format == \"channels_first\":\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    input_shape = (28, 28, 1)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "scores = mnistutils.train_model(model, data, batch_size=256, epochs=10)\n",
    "print(\"Scores on test set: loss=%s accuracy=%s\" % tuple(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('conv-mnist-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('conv-mnist-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wo_softmax = iutils.keras.graph.model_wo_softmax(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_digit(x):\n",
    "    plt.imshow(x.reshape((28, 28))) \n",
    "    plt.axis('off')\n",
    "plot_digit(data[0][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLift(ReverseAnalyzerBase):\n",
    "    \n",
    "    def _default_reverse_mapping(self, Xs, Ys, reversed_Ys, reverse_state):\n",
    "        return ilayers.GradientWRT(len(Xs))(Xs+Ys+reversed_Ys)\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        # initialize the reference input (black image)\n",
    "        x_0 = np.zeros([1] + list(K.int_shape(model_wo_softmax.input)[1:]))\n",
    "        \n",
    "        # get input tensor for every kernel layer ()\n",
    "        intermediate_inputs = []\n",
    "        layers_with_kernel = []\n",
    "        for l in self._model.layers:\n",
    "            if kchecks.contains_kernel(l):\n",
    "                layers_with_kernel.append(l.name)\n",
    "                intermediate_inputs.append(l.input)\n",
    "\n",
    "        # ignore the first input layer, also include the last output\n",
    "        # for adjusting the relevence score before propagating\n",
    "        intermediate_inputs = intermediate_inputs[1:] + [model_wo_softmax.output]\n",
    "        \n",
    "        self.rootpoints_for_kernel_layers = dict(\n",
    "                zip(\n",
    "                    layers_with_kernel + ['rel_score'],\n",
    "                    [x_0] + K.function([model_wo_softmax.input], intermediate_inputs)([x_0])\n",
    "                )\n",
    "            )\n",
    "#       print('We have %d int. inputs' % (len(self.rootpoints_for_kernel_layers)))\n",
    "        \n",
    "\n",
    "    def _get_state(self):\n",
    "        state = super(ReverseAnalyzerBase, self)._get_state()    \n",
    "        state.update({\n",
    "            'rootpoints_for_kernel_layers': self.rootpoints_for_kernel_layers\n",
    "        })\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _create_analysis(self, *args, **kwargs):\n",
    "        # TODO: Adjust the last layer with relevance of the reference input\n",
    "        \n",
    "        rootpoints = self._get_state()['rootpoints_for_kernel_layers']\n",
    "        \n",
    "        for l in self._model.layers:\n",
    "            if l.name in self.rootpoints_for_kernel_layers:\n",
    "                l._deeplift_rootpoint = rootpoints[l.name]\n",
    "\n",
    "        self._add_conditional_reverse_mapping(\n",
    "            lambda layer: kchecks.contains_kernel(layer),\n",
    "            DeepLiftRule,\n",
    "\n",
    "            name=\"deeplift_rule\",\n",
    "        )\n",
    "        \n",
    "        return super(DeepLift, self)._create_analysis(*args, **kwargs)\n",
    "    \n",
    "class DeepLiftRule(kgraph.ReverseMappingBase):\n",
    "    \"\"\"                                                                                                                                                                              \n",
    "    Basic DeepLiftRule decomposition rule (for layers with weight kernels),                                                                                                                   \n",
    "    which considers the bias a constant input neuron.                                                                                                                                \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, state, rootpoints=None, bias=True):\n",
    "        # Copy forward layer, but without activations\n",
    "        self._layer_wo_act = kgraph.copy_layer_wo_activation(layer,\n",
    "                                                             keep_bias=bias,\n",
    "                                                             name_template=\"reversed_kernel_%s\")\n",
    "        \n",
    "        self.Xs_root = layer._deeplift_rootpoint\n",
    "        xs_root_input = keras.layers.Input(shape=self.Xs_root.shape[1:])\n",
    "        self.Zs_root = K.function([xs_root_input], kutils.apply(self._layer_wo_act, [xs_root_input]))([self.Xs_root])[0]\n",
    "\n",
    "\n",
    "    def apply(self, Xs, Ys, Rs, reverse_state):\n",
    "        grad = ilayers.GradientWRT(len(Xs))\n",
    "\n",
    "        Zs = kutils.apply(self._layer_wo_act, Xs)\n",
    "        \n",
    "        Zs_adjusted = [keras.layers.Lambda(lambda z: z - self.Zs_root)(Zs[0])]\n",
    "        Xs_adjusted = [keras.layers.Lambda(lambda z: z - self.Xs_root)(Xs[0])]\n",
    "        \n",
    "        # Divide incoming relevance by the activations.    \n",
    "        tmp = [ilayers.SafeDivide()([a, b])\n",
    "               for a, b in zip(Rs, Zs_adjusted)]\n",
    "        \n",
    "        # Propagate the relevance to input neurons                                                                                                                                   \n",
    "        # using the gradient.               \n",
    "        tmp = iutils.to_list(grad(Xs+Zs+tmp))\n",
    "\n",
    "        # Re-weight relevance with the input values.                                                                                                                                 \n",
    "        return [keras.layers.Multiply()([a, b])\n",
    "                for a, b in zip(Xs_adjusted, tmp)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplift_analyzer = DeepLift(model_wo_softmax)\n",
    "deeplift_analyzer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3_input (InputLayer)  (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 4,743,050\n",
      "Trainable params: 4,743,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deeplift_analyzer._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = deeplift_analyzer.analyze(data[0][20,:].reshape((1, 28, 28, 1)))\n",
    "plot_digit(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAADNJJREFUeJzt3Vts5GUZx/HnP9OZTqfttLtt9wjsKq50IeCBg+AagxeKRhITRQ4xxMR4wYUXXKAXxES4UkM00QtxjReiXhi4ERMMiBEDhqhElIMu7Lqwu93t9nxup3P6/73glvf3IkPbaZ/v55Jn3850pr95E5553jfJsswA7Hy5rX4CADYHYQecIOyAE4QdcIKwA050beaDfTr3Zf7XP7DBnk4fS97uv7OzA04QdsAJwg44QdgBJwg74ARhB5wg7IATm9pnh0O5/NY9dpaKmr+vfLCzA04QdsAJwg44QdgBJwg74ARhB5wg7IATO6fPnrztCO97x2Ff1syir2uS1330pLs7XCuFa2Zm1hX582y1ZDmrrgdrqai99Q/0z96O2NkBJwg74ARhB5wg7IAThB1wgrADTnRW662NNk8Sa9NEWkTRNk4rPC6ZRdZu5zZOrLWWK5f1D9g3Eiy1BvXatFu/p82yfm6lybVgLXfyjH7salXWt2Mrlp0dcIKwA04QdsAJwg44QdgBJwg74ARhB5zoqD570lWQ9Vxfb3jtYEWuTft79IPn9OdefnE1/LOnZvRjx8Yp1ZHHZu31dNsd/Y2NsA7tkvXZ68N99v7zNbl2ba8egc0iv9rSoYFgbd/8kP7ZFy7qerOpH7wDsbMDThB2wAnCDjhB2AEnCDvgBGEHnCDsgBOb22ePXN+b6ynJenZwb7DWrOiebH2wKOtLh/RLkV8P92xHno98P2BsXNbTdd1vNov04duR6M/7pKhft+be8OtiZjZ7dbgZvvv3Z+Tan//zCVm/933HZH3uwRuDtdaIft7J5LSsR88w6MB5d3Z2wAnCDjhB2AEnCDvgBGEHnCDsgBOEHXCis+bZ+/tkffVweGa958kX5drL/6J7+Bc/p3vlr//4cLCWa4Vnts3MhhuR2efxCVlO6w29PjYP34akW/fZ10f069ozuhCsTdwxKtceLT4j69f8Q5Zt+mfhWmNAfy+jWNB/DxZ9TzrvrgB2dsAJwg44QdgBJwg74ARhB5wg7IATHdV6s4J+OrXB8GdT5KBo+8KQbs09+sQNsn7goXAL6uJt+qjotGu/rO/5syzHW3O12IhsWOxK5qRbt6hW9+j11+47H6y9OaZbrSfq4SuXzcxeXw6PPJuZ5UT3q17Rf2vdxUjrLXyyeMdiZwecIOyAE4QdcIKwA04QdsAJwg44QdgBJzqrz97So5ppV/hY4izVR/c+PvtRWf/WgSdl/Rv3hK8mLj6r++iLn1mR9e5F3S+uLOumbiKuD44eeZyL3Hsc6TdX9+j1L0+HX5v6lZGx4sYeWT85qUeLD/8iPAO79EX995DERly3IXZ2wAnCDjhB2AEnCDvgBGEHnCDsgBOEHXCio/rsWeTq4tJ8uGfcdekBufaZ13TP9vKyvqL3kdFfBWu3/+abcq3dpOfdJ27slfW+N/RzT1ZFH75e12sj8+xZ5IyBZp/+fkP9ueFg7cbbXpJr/7R4VNZrq/qYayWN/eUnke8fbEPs7IAThB1wgrADThB2wAnCDjhB2AEnCDvgxOb22SNXC2fruh9dmg734c/eealce+Rrf5P1Xz96vawfunomWHvo28fl2u9/6U5Zrz0QvtbYzGzpgwOyPjgmTs2P9IuTkj4XPivreqrb9LZ6Sfi7Ec/+4Rq5dvj6Sf3D63qvmvnqtcFaV1V/PyBrdt6Vy+1iZwecIOyAE4QdcIKwA04QdsAJwg44QdgBJ7bVPHvhYrgf3TNVlmvPfudjst77tO67Hu//ZLD29UPPybWn7q7Ielddnws/f4X+TB58ZShYyy3rO86zfv26VffrWftmRfejB06E/8QWrwqfd29mNj3fL+uWRmbORblrXb/f1tDnAGxH7OyAE4QdcIKwA04QdsAJwg44QdgBJzZ5xDUyVhi5XjidCo+ZDrwZbj+Zma1cVpL1fU+el/UTx8LXA780fJlc++Ctj8n6A4/fLuv9183K+uRE+Hcvz4SvmjYzyyIf9/1P/UfWj76gj3NujIZHj9cOitFcM2vW9XuWr737vSrR09Y7Ejs74ARhB5wg7IAThB1wgrADThB2wAnCDjjRUSOuluo+e1YLj8AWL+jjmHsv7NWP3dKN12Q23E8+vRLuwZuZHS7pPvmRR+Zk/fOPPS/rv6zeGqz1PfWqXJuq657N7Px9H5f1mJx4S3v1VxusOqLPqR48qd+z1YPhvay4HBmPLejvDyQ5vT5yavqWYGcHnCDsgBOEHXCCsANOEHbACcIOOEHYASc6q88ekaViHn5pRa7tnRyW9XN3HZL10R+Fm8Iv3a/n2Rdqem678pNFWf/txQ/L+uxnw1ddL1yhr0UurOh+cVqQZWsV9RkFuUb45/dM67X5mn5u80d1/QPHzwVrMzfrK76Tsp6lt4XIPhm5Kjt2tsNGYGcHnCDsgBOEHXCCsANOEHbACcIOOEHYASe2VZ9dDQlny7rPXj67JOtrw/p89bXR8Dx8zzndjB5b2Sfraa+e48/3N2RdtWzr+/XaVkk/drk33MM3M2usdct6czLcr841IntNu63oRvh3j31/ICtG/sE2xM4OOEHYAScIO+AEYQecIOyAE4QdcIKwA05ssz57uPGa1etyaW5Sn83eO9Ev6/lauB+dtHRPNjaXnRX0Z24u0Q3nUl/4PP1aIfLcXumT9VaqZ/GPPHxCr18In+c/ft9Ncm3s7vh8NTIzLrT0sfBmsT575Nz4TsTODjhB2AEnCDvgBGEHnCDsgBOEHXBie7XeBHnMtJlZVY9qFhd16y4RP796QI+J7j8yLev5SGtt7LS+Erryx3L4sf+tr2Sevl9fJ91K9X4w94khWa//bjRYS/TLFh1xzbdxLXKrO3KEdklHI4kcFb35B0XHsbMDThB2wAnCDjhB2AEnCDvgBGEHnCDsgBM7ps+ujpk2M8takeOaV3WfvXowPAp69IcTcu3Cdfoo6f6/XpD1vjv02zR/czVYm/uQHlEtvaBHXAvLsmyLFV0fmWgGa7NH9e+V06dgR8dUp295f7AW+WqD1Ycir1sx8uD1yJPPYl8yeO+xswNOEHbACcIOOEHYAScIO+AEYQecIOyAEzunzx4T6bMnVd1nb/Tlg7U37j4o19Z36e8AjH/qgKwX53RTePcz4WuRB0/rOf7CRKSRHnndVq7Ss/aF5fD62lDkCO1mZOY8/JaYmdlCb7hWXNRrly/RR0n37BqQ9SRyfkKW0mcHsEEIO+AEYQecIOyAE4QdcIKwA04QdsAJN332rKV73clc+GphM7PKye5grbAiGrpm1izrz9SutciVzNMrsp6fCj/3bEWfG59F5q6TUriHb2ZWWN4t6/XB8J9Yz6R+XerX6d87F5lJbyyH37Mjx/UZBGe+cqmsN/fvkvXcpL4rIFPfIRBXk7eDnR1wgrADThB2wAnCDjhB2AEnCDvgxM5pvUXaFVlTt5jSRT3qmdTCI7Dl85FjhZPIZ2pk3DHWHksb4XrWDB/l/I6kumVZmNejnHNXhkdBd7+mn9tUWR9zvffYuKzvPhBuO77yXT2WXHpRlq0+oN/zUkGPyFqtpusbgJ0dcIKwA04QdsAJwg44QdgBJwg74ARhB5zYOX32mDb78NlquBeeVMNXJr/1DyKfqbHrptPIyGNkfVsiffr8tB4N7rsQ7pWPH9NnQQ+/rH+vc0N7Zf2eWx4N1kZKenz27A/C1z2bmaWlSB+9A7GzA04QdsAJwg44QdgBJwg74ARhB5wg7IATfvrsMbHje7Nwn30j29xmZpboq4s3UtbQffZ0ZlbWK/8K/4m1ivvl2pmP6N979Kdzsv69U3cFa/se/rtcu3Rbv6xXTkWuuo6cA7AV2NkBJwg74ARhB5wg7IAThB1wgrADThB2wAn67NvBBl3h+87ofnEaOdPepmaCpcFX9XXQzR59HfR/7x6S9aQZft0u3HuDXDtwRp/ln1tck/W03fP6NwA7O+AEYQecIOyAE4QdcIKwA04QdsAJwg44QZ8dWrTHHznzXtxDnoxdlGuHm7rXXTlbkfVmT/hc+sKy7oMXJpdk3ab0HH/sHICt+O4EOzvgBGEHnCDsgBOEHXCCsANOEHbACVpvaE/sKuxWuH2Wrukx0WRsXNaLk9O6rq7KFs/LzCyr12U9Otqb6p+/FdjZAScIO+AEYQecIOyAE4QdcIKwA04QdsAJ+uzYWKIPn0WOW1Y9ejMzWw+Pz7Ytdg/3lh7v/e6wswNOEHbACcIOOEHYAScIO+AEYQecIOyAE0m2DfuFAP5/7OyAE4QdcIKwA04QdsAJwg44QdgBJwg74ARhB5wg7IAThB1wgrADThB2wAnCDjhB2AEnCDvgBGEHnCDsgBOEHXCCsANOEHbACcIOOEHYAScIO+DE/wC5ae4OG6wmTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dtd = DeepTaylor(model_wo_softmax)\n",
    "ax = dtd.analyze(data[0][20,:].reshape((1, 28, 28, 1)))\n",
    "plot_digit(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in deeplift_analyzer._reversed_tensors:\n",
    "    print(k, np.min(v), np.max(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_1_input:0' shape=(?, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in analyzer._reversed_tensors:\n",
    "    print(k, np.min(v), np.max(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(1)])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplift_analyzer._model.input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.Conv2D object at 0x14c5a9780>\n",
      "<keras.layers.convolutional.Conv2D object at 0x14c5a9668>\n",
      "<keras.layers.core.Dense object at 0x14c5a9ac8>\n",
      "<keras.layers.core.Dense object at 0x13bd66978>\n",
      "We have 4 int. outputs\n"
     ]
    }
   ],
   "source": [
    "intermediate_outputs = []\n",
    "for l in model_wo_softmax.layers:\n",
    "    if kchecks.contains_kernel(l):\n",
    "        print(l)\n",
    "        intermediate_outputs.append(l.input)\n",
    "\n",
    "print('We have %d int. outputs' % (len(intermediate_outputs)))\n",
    "get_intermediate_outputs = K.function([model_wo_softmax.input], intermediate_outputs[1:])\n",
    "\n",
    "int_outputs = get_intermediate_outputs([x_zeros])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv2d_3_9/Relu:0' shape=(1, 26, 26, 32) dtype=float32>]"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kutils.apply(model_wo_softmax.layers[1], [K.variable(x_zeros)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv2d_3_10/Relu:0' shape=(?, 26, 26, 32) dtype=float32>]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kutils.apply(model_wo_softmax.layers[1], [model_wo_softmax.input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_38:0' shape=(1, 28, 28, 1) dtype=float32_ref>"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Input(tensor=K.variable(x_zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 28, 28, 1)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.layers[1].input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24, 24, 64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_3rd_layer_output = K.function([model_wo_softmax.input],\n",
    "                                  [model_wo_softmax.layers[3].input])\n",
    "get_3rd_layer_output([x_zeros])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_zeros = np.zeros((1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.int_shape(model_wo_softmax.input)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer conv2d_3 has kernel\n",
      "Layer conv2d_4 has kernel\n",
      "Layer dense_4 has kernel\n",
      "Layer dense_6 has kernel\n",
      "We have 5 int. inputs\n"
     ]
    }
   ],
   "source": [
    "# x_0 = np.zeros([1] + list(K.int_shape(model_wo_softmax.input)[1:]))\n",
    "x_0 = data[0][0,:].reshape((1, 28, 28, 1))\n",
    "intermediate_outputs = []\n",
    "layers_with_kernel = []\n",
    "for l in model_wo_softmax.layers:\n",
    "    if kchecks.contains_kernel(l):\n",
    "        layers_with_kernel.append(l.name)\n",
    "        print('Layer %s has kernel' % l.name)\n",
    "        intermediate_outputs.append(l.input)\n",
    "\n",
    "intermediate_outputs = intermediate_outputs[1:] + [model_wo_softmax.output]\n",
    "get_intermediate_outputs = K.function([model_wo_softmax.input], intermediate_outputs)\n",
    "\n",
    "rootpoints_for_kernel_layers = dict(zip(layers_with_kernel + ['rel_score'], [x_0] + get_intermediate_outputs([x_0])))\n",
    "print('We have %d int. inputs' % (len(rootpoints_for_kernel_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootpoints_for_kernel_layers['rel_score'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv2d_3'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.layers[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3_input (InputLayer)  (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 4,743,050\n",
      "Trainable params: 4,743,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_wo_softmax.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_6/BiasAdd:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.function([model_wo_softmax.input], [model_wo_softmax.output])([np.zeros((1, 28, 28, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'dense_6/BiasAdd:0' shape=(?, 10) dtype=float32>]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wo_softmax.output == model_wo_softmax.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3-Main",
   "language": "python",
   "name": "python-3-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
